from tensorflow.keras.layers import Attention

def build_attention_model(input_shape):
    model = Sequential([
        Input(shape=input_shape),
        Attention(),
        Flatten(),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])
    return model

# Example training code
X_train = np.random.rand(1000, 20, 5)  # Replace with actual training data
y_train = np.random.randint(0, 2, 1000)  # Binary labels: no reversal (0) / reversal (1)
attention_model = build_attention_model((20, 5))
attention_model.fit(X_train, y_train, epochs=10, batch_size=32)
attention_model.save('models/attention.h5')
